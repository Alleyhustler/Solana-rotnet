<!DOCTYPE html><!--┴─┬─┴─┤ᴥ•ʔ ├─┬─┴─┬─┴--><html lang=en>
<!-- Mirrored from ve3zsh.neocities.org/blog/2021/08/21/uuids_and_b_trees_and_legacy_oh_my by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 20 May 2025 11:00:25 GMT -->
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#073642"><link rel=canonical href=https://ve3zsh.ca/blog/2021/08/21/uuids_and_b_trees_and_legacy_oh_my.html><link rel=icon href=../../../../favicon.ico sizes=16x16><link rel=icon href=../../../../favicon.svg type=image/svg+xml><link rel=stylesheet href=../../../../core.css><link rel=stylesheet href=../../../../print.css media=print><script src=https://ve3zsh.neocities.org/api.js defer></script><script src=../../../../core.js defer></script><link rel=alternate href=../../../index.xml type=application/rss+xml><link rel=preload href=../../../../code.woff2 as=font type=font/woff2 crossorigin><script src=../../../../tag/egg.js defer></script><title>UUIDs and B-Trees and Legacy, Oh My!</title>
<meta name=author content="Keller"><meta name=description content="An adventure in improving the performance of a legacy system."><header id=top class=no-print><a id=logo href=../../../../index.html><img src=../../../../img/logo.png alt=VE3ZSH height=35 width=120></a><div class=spacer></div><nav id=menu><span class=no-css>Menu:</span>
<a href=../../../../index.html class=menu-item><img src=../../../../img/icons/projects.svg alt decoding=async height=0 width=0>Projects</a> <span class=no-css>|</span>
<a href=../../../index.html class=menu-item><img src=../../../../img/icons/writings.svg alt decoding=async height=0 width=0>Writings</a> <span class=no-css>|</span>
<a href=../../../../links/index.html class=menu-item><img src=../../../../img/icons/links.svg alt decoding=async height=0 width=0>Links</a> <span class=no-css>|</span>
<a href=../../../../contact/index.html class=menu-item><img src=../../../../img/icons/contact.svg alt decoding=async height=0 width=0>Contact</a></nav></header><hr class=no-css><main><article><h1><abbr title="Universally Unique Identifiers">UUIDs</abbr> and B-Trees and Legacy, Oh My!</h1><p>I recently did a bit of work to improve the performance of a system. Normally this isn't the sort of
thing I'd write about, but this particular adventure has a lot of interesting things I thought you
might enjoy reading about. The system itself isn't all that special. It's a piece of middleware for
an information system. It sits between a user's application and a database offering a
<abbr title="Create, Read, Update, Delete">CRUD</abbr>
<abbr title="Application Programming Interface">API</abbr> to clients.<p>I think this can serve as a bit of a case study for what I keep trying to tell people, which is you
cannot just "optimize" a system built without a care for performance at the end. While we were able
to improve the performance issue, it's still troubled. There are many deeply embedded performance
issues with the system that remain effectively unfixable. If you build software without a constant
eye towards properties like performance and security throughout the process, you will end up with an
insecure and/or inefficient system that you cannot just magically improve at the end. It remains
structurally deficient.<p>With that out of the way, I hope you enjoy the read.<h2><a id=identifiers href=#identifiers>Identifiers (<abbr title="Universally Unique Identifier">UUID</abbr>s)</a></h2><p>The particular design of the system that matters here is that the original authors chose to use
<abbr title="Universally Unique Identifiers">UUIDs</abbr> for record identification. All information
systems require every record be given an identifier so that it can be referenced and retrieved. This
is true no matter the implementation of the system, from analogue library, to console video game, to
even something as massive as the internet itself. All of these information systems require resource
identifiers. This can be as simple as an index into an array or as complicated as a hierarchically
coordinated identifier issuing agency like
<abbr title="International Standard Book Number">ISBN</abbr> or
<abbr title="Internet Assigned Numbers Authority">IANA</abbr>'s
<abbr title="Internet Protocol">IP</abbr> address assignments. No matter the scale and forethought,
all information systems require identifiers for resources.
<abbr title="Universally Unique Identifiers">UUIDs</abbr> are one way you can assign identifiers to
resources within an information system.<p>So how does choosing <abbr title="Universally Unique Identifiers">UUIDs</abbr> for identifiers lead
to a performance issue? Well, let's first look at what
<abbr title="Universally Unique Identifiers">UUIDs</abbr> are.
<abbr title="Universally Unique Identifiers">UUIDs</abbr> were first designed to be used within the
<a href="https://www.youtube.com/watch?v=CcLMFpYmsB8">Apollo Network Computing System (<abbr title="Network Computing System">NCS</abbr>)</a>
in the 1980s. The <abbr title="Network Computing System">NCS</abbr> was an abstraction designed by
Apollo Computers to make writing software for distributed systems resemble writing software for a
single computer. <abbr title="Universally Unique Identifiers">UUIDs</abbr> were specifically
designed so that any number of computers can simultaneously generate identifiers that are guaranteed
to be unique among all identifiers generated by all computers. Later these made their way into the
Distributed Computing Environment (<abbr title="Distributed Computing Environment">DCE</abbr>)
system under supervision of the Open Software Foundation
(<abbr title="Open Software Foundation">OSF</abbr>) as
<a href=https://www.iso.org/standard/2229.html><abbr title="International Organization for Standardization">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 11578:1996</a>.
Apollo would be acquired by Hewlett-Packard in 1989 but the technology would live on to ultimately
be standardized by <a href=https://datatracker.ietf.org/doc/html/rfc4122>RFC 4122</a> in 2005.<p>A <abbr title="Universally Unique Identifier">UUID</abbr> has a fairly common and recognizable
representation as a hexadecimal string grouped with hyphens, for example
<code>01234567-0123-0123-0123-0123456789ab</code>. There's just one catch,
<abbr title="Universally Unique Identifiers">UUIDs</abbr> are not just one specification, but
instead are generated using one of five different algorithms. For the particular system in question,
the version of <abbr title="Universally Unique Identifier">UUID</abbr> in use is the version 4, or
purely random <abbr title="Universally Unique Identifier">UUID</abbr> generation algorithm. In this
version, the representation takes the form <code>________-____-4___-[89ab]___-____________</code>
where <var>_</var> represents any hexadecimal nibble (<code>[0-9a-f]</code>). Formally:<blockquote><p>The version 4 <abbr title="Universally Unique Identifier">UUID</abbr> is meant for
generating <abbr title="Universally Unique Identifiers">UUIDs</abbr> from truly random
or pseudo-random numbers.<p>The algorithm is as follows:<ul><li>Set the two most significant bits (bits 6 and 7) of the <var>clock_seq_hi_and_reserved</var>
to zero and one, respectively.<li>Set the four most significant bits (bits 12 through 15) of the <var>time_hi_and_version</var>
field to the 4-bit version number from
<a href=https://datatracker.ietf.org/doc/html/rfc4122#section-4.1.3>Section 4.1.3</a>.<li>Set all the other bits to randomly (or pseudo-randomly) chosen values.</ul><footer>— <cite><a href=https://datatracker.ietf.org/doc/html/rfc4122#section-4.4>RFC 4122 section 4.4</a></cite></footer></blockquote><h2><a id=indexes href=#indexes>Indexes (B-Trees)</a></h2><p>Why is this important? Well, the system requires efficient lookups of this identifier to return the
associated resource. The problem is that these identifiers are handed out randomly, each new
identifier issued has as much a chance to be lexically early as it does to be late. Another way to
say that is the chance the lead nibble is a <code>0</code> is just as likely as it is to be an
<code>f</code> and this is true for each successive nibble. This by itself isn't an issue, but it is
when you need to use an index like a B-Tree.<p>An index if you're unaware is just a data structure that has efficient lookup and maintenance
complexities. You're probably aware of runtime complexities like <code>O(n)</code>. This complexity
for example is the naive complexity of just looking through every single record for the one with a
given identifier. It's not bad, but not fast enough when your system holds billions of records and
you need a latency in the millisecond range. To do better, all databases implement one or more
indices. In the case of the existing database, this index was a B-Tree. B-Trees provide
<code>O(log<sub>k</sub>(n))</code> lookups at the cost of <code>O(n + log(n))</code> additional
storage and an <code>O(n)</code> write performance penalty.<p>You may wonder why we're willing to pay <code>O(n)</code> to write data (as well as that additional
storage) when we won't pay that much to read the data. Well, it's a matter of what that <var>n</var>
refers to. In the case of reads, that's every record that exists in the system. In the case of
writes, that only represents the number of records we write at a given time. Given the system will
be frequently searching for these records, the scale of the write penalty is returned by the read
performance many times over. Just remember that there is a price to having an index and you should
always make these trade-offs based on the circumstances and not just dogma.<p>Anyway, back to why B-Trees and <abbr title="Universally Unique Identifiers">UUIDs</abbr> don't mix.
The problem is that B-Trees are inherently lexicographic. Invented in 1970 by
<a href=https://wwwbayer.in.tum.de/cgi-webcon/webcon/lehrstuhldb/details/Mitarbeiter/num/5/1>Rudolf Bayer</a>
and <a href=https://www.mccreight.com/people/ed_mcc/index.htm>Edward M.
McCreight</a> while
working at Boeing, B-Trees are essentially a highly optimized binary tree. For simplicity let me
explain how a binary tree works and then how it differs from a B-Tree.<p>In a binary tree, nodes are defined such that each node has 3 parts,<ol><li>its value,<li>a link to a child node of lower value, and<li>a link to a child node of higher value.</ol><figure><div class=no-css></div><img src=bintree.svg alt="Example binary tree" loading=lazy height=535 width=820><div class=no-css></div><figcaption>Example of a binary tree.</figcaption></figure><p>To search for a given identifier you start at the root node. You compare the identifier you're
looking for to the node's value. Assuming this node doesn't have the identifier you're looking for,
if your identifier is bigger than the value of the node you follow the link to the higher node, if
it's smaller you follow to the lower. This subdivides the remaining nodes such that on average you
only need to look at <code>O(log<sub>2</sub>(n))</code> nodes to find the node you're looking for.
If you then store in each of those nodes the location in the table for the record itself, you'd have
a working index for your data.<p>B-Trees only differences are that it's self balancing and you have many more elements per node. Self
balancing just means changes to the tree (like adding a new record) can cause it to spend some time
moving nodes around to ensure the tree has a nearly uniform depth. This ensures the search path is
as short as possible for all nodes on average. The other difference, having more elements per node,
is a simple concept that requires a bit more to understand.<figure><div class=no-css></div><img src=btree.svg alt="Example B-tree" loading=lazy height=388 width=808><div class=no-css></div><figcaption>Example of a B-tree.</figcaption></figure><p>Instead of a single value and lower/higher links it keeps an array of values and an array of links
who's length is one longer than the values array. These arrays align such that they could be
interpreted interleaved as first a link to a lower node, then a link between each adjacent pair of
values, and finally a link to a higher node. When searching for an identifier you start at the root
node as before but instead scan the values array for the two elements that bound the identifier
you're looking for. This picks the link that "sits between them." These nodes are designed so that
all of the values in the linked node are between the values that bound the link. This is where the
<var><sub>k</sub></var> comes from in the complexity mentioned earlier. Each node has
<code>k - 1</code> elements so that searching a node reduces the search space by <var>k</var>. This
presents a puzzle though, doesn't this mean the computer is doing more work than it was in a binary
tree?<p>Yes! Searching a B-Tree means at each node there's a linear scan of the values for the bounds of the
identifier. This looks at way more values than you would if we followed a strictly balanced binary
tree. However, this is where reality kicks theory's butt. This value of <var>k</var> is chosen
specifically based on the machine the B-Tree is running on to take as much advantage of things like
the cache line and page size as possible. See, computers are not abstract pointer machines, but
instead exhibit real performance characteristics based on data locality, a concept completely
excluded from Big-O complexity models. In reality, following a link to a node elsewhere in memory is
far more expensive than a linear search through a cache line's worth of data. B-Trees exploit this
by packing the cache line.<p>That doesn't really sound like a problem for
<abbr title="Universally Unique Identifiers">UUIDs</abbr> though. B-Trees should be able to handle
<abbr title="Universally Unique Identifiers">UUIDs</abbr> like any other identifier, shouldn't it?
Well, the problem is what happens once you start to run out of memory. Again, in reality this all
has to live in memory on a system that only has so much. This memory is precious because of how much
faster it is than persistent storage (where everything ultimately lives). The problem is, once the
database starts to run out of memory for cached data, indexes, and its own process data/text, it has
to make choices about what gets to sit in memory and what gets evicted to make room. Once evicted,
it will have to be read back in before it can be used. The problem is that in a system with random
identifiers, every single node is equally likely to be needed and so the entire index has to be in
memory even though say 80% of the data is really old and not likely to be needed any time soon.<h2><a id=picking-a-path href=#picking-a-path>Picking a Path</a></h2><p>Thus the performance issue is found. Now what? Well, now real effort comes in picking trade-offs on
how to solve it. See, <abbr title="Universally Unique Identifiers">UUIDs</abbr> were not a great
choice. Firstly, this system does not need to distribute identifier generation. Sure the middleware
runs on more than one machine, but the database is on a single server. Give the task of identifier
generation to a single machine (like the database) and you don't need the complexities of
universality. <abbr title="Universally Unique Identifiers">UUIDs</abbr> also eat a significant
amount of space compared to just about any other alternative. For example, you can hand out well
over a billion sequential IDs using just a 32 bit integer. A
<abbr title="Universally Unique Identifier">UUID</abbr> is at best 128 bits, however, this system
isn't storing them as efficiently as possible. Instead it chose to store them as the hyphenated
hexadecimal string you saw which means they each take up 288 bits or 2.25 times as much space as
they really needed to.<p>Problem is though, the system is fairly large and that identifier is a part of the interface it
exposes through its <abbr title="Application Programming Interface">API</abbr>. We cannot simply get
rid of them without rewriting large parts of the system and the clients that depend on it. If
changing the identifier is expensive, the alternative then is to change the index. Well,
<a href=https://dev.mysql.com/doc/refman/5.6/en/create-index.html#create-index-storage-engine-index-types>My<abbr title="Structured Query Language">SQL</abbr> unfortunately doesn't offer any other index type</a>.
Yeah, our hope at this point is moving to a hash index. The problem is we're going to have to change
databases to do it.<p>Postgre<abbr title="Structured Query Language">SQL</abbr> has technically had support for hash
indexes since version 7.2 in <time datetime=2002-02>February 2002</time>, but it wasn't production
ready until <time datetime=2017-10>October 2017</time> with the release of version 10. The page
for index has had a prominent though changing note regarding the usage of hash indexes.<blockquote><p><b>Note:</b> Because of the limited utility of hash indexes, a B-tree index should generally be
preferred over a hash index. We do not have sufficient evidence that hash indexes are actually
faster than B-trees even for = comparisons. Moreover, hash indexes require coarser locks.<footer>— <cite><a href=https://www.postgresql.org/docs/7.2/indexes-types.html>Postgre<abbr title="Structured Query Language">SQL</abbr> 7.2</a>
</cite>, <time datetime=2002-02>February 2002</time>.</footer></blockquote><blockquote><p><b>Note:</b> Testing has shown Postgre<abbr title="Structured Query
		Language">SQL</abbr>’s hash indexes to be similar or slower than B-tree indexes, and the
index size and build time for hash indexes is much worse. Hash indexes also suffer poor
performance under high concurrency. For these reasons, hash index use is discouraged.<footer>— <cite><a href=https://www.postgresql.org/docs/7.3/indexes-types.html>Postgre<abbr title="Structured Query Language">SQL</abbr> 7.3</a>
</cite>, <time datetime=2002-11>November 2002</time>.</footer></blockquote><blockquote><p><b>Note:</b> Testing has shown
Postgre<abbr title="Structured Query Language">SQL</abbr>’s hash indexes to
perform no better than B-tree indexes, and the index size and build time for hash indexes is
much worse. Furthermore, hash index operations are not presently
<abbr title="Write-ahead Log">WAL</abbr>-logged, so hash indexes may need to be
rebuilt with <code>REINDEX</code> after a database crash. For these reasons, hash index use is
presently discouraged.<p>…the problems with hash indexes may be fixed eventually…<footer>— <cite><a href=https://www.postgresql.org/docs/8.1/indexes-types.html>Postgre<abbr title="Structured Query Language">SQL</abbr> 8.1</a>
</cite>, <time datetime=2005-11>November 2005</time>.</footer></blockquote><blockquote><p><b>Caution:</b> Hash index operations are not presently
<abbr title="Write-ahead Log">WAL</abbr>-logged, so hash indexes might need to be
rebuilt with <code>REINDEX</code> after a database crash if there were unwritten changes.
Also, changes to hash indexes are not replicated over streaming or file-based replication
after the initial base backup, so they give wrong answers to queries that subsequently use
them. For these reasons, hash index use is presently discouraged.<footer>— <cite><a href=https://www.postgresql.org/docs/9.0/indexes-types.html>Postgre<abbr title="Structured Query Language">SQL</abbr> 9.0</a>
</cite>, <time datetime=2010-10>September 2010</time>.</footer></blockquote><p>Later in
<a href=https://www.postgresql.org/docs/10/indexes-types.html>Postgre<abbr title="Structured Query Language">SQL</abbr> 10</a>,
the warning was dropped. However, there are still many reasons not to use a hash index unless you
really need it. A read through the new and dedicated
<a href=https://www.postgresql.org/docs/current/hash-intro.html>hash index docs</a> lists many
limitations like,<ul><li>supporting only single-column indexes,<li>not affording uniqueness checking,<li>supporting only the = operator, and<li>expansion occurring in the foreground which could increase insert execution times.</ul><p>This page is well worth a read if you're in a similar situation because it's not an upgrade but an
alternative with considerations and trade-offs. Again, death to dogma in software development. In
our case though it makes good sense to go this route.<p>On how the hash index works, it's similar to how a hash table works. I have to say similar hear
because hash tables are still an active area of research. There are a large number of subtle
variations in implementation that can deeply impact performance and utility. In
Postgre<abbr title="Structured Query Language">SQL</abbr>'s case, values go through a function that
projects the input to an integer. This integer is used to calculate an offset into a series of
buckets. Each bucket is a collection of pages that contain some number of hashed values that each
map to the row in the table to which it belongs. The values within a page are kept in sorted order
to allow a binary search for a given value. Pages within a bucket are chained as a linked list. When
a bucket gets too full Postgre<abbr title="Structured Query Language">SQL</abbr> automatically
splits the bucket to reduce the number of pages and values it has to search in a given bucket. A
full description of the particulars can be found in
<a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=blob;f=src/backend/access/hash/README;hb=HEAD">Postgre<abbr title="Structured Query Language">SQL</abbr>'s Hash README</a>
but that's essentially it. Complexity of <code>O(1)</code> without lexicographic ordering. Sadly the
entire index still has to fit in memory, but because
Postgre<abbr title="Structured Query Language">SQL</abbr>'s hash indexes don't store the identifier,
but only the hashed output, they can take up significantly less memory in our case.<h2><a id=actually-cutting-over href=#actually-cutting-over>Actually Cutting Over</a></h2><p>So we have a plan, just up and replace the database. Easy, right? Well, easier than the alternative.
In all, it takes over a month of work to cut over. There are a couple tiny differences between
My<abbr title="Structured Query Language">SQL</abbr> and
Postgre<abbr title="Structured Query Language">SQL</abbr> that require modifying the code, but all
of these are internal details to the service that don't impact clients or the
<abbr title="Application Programming Interface">API</abbr>.<p>If you've never migrated a live middleware system before, there's a fairly standard technique you
use. Essentially you have your service:<ol><li>Begin writing to the new location on top of writing to the old.<li>Start a process to migrate all the old data to the new location.<li>Start reading from the new location.<li>Stop writing to the old location.</ol><p>In our case we used a program that would copy the contents of the old database to the new, but also
follow the binary log from the old database to catch up with all the changes to it and mirror those
as well. This let us write to only the old database the whole time. We then combined the last two
stages by using a runtime flag to change the database all at once. This required leaving gaps in
auto incrementally number identified records so that any last second changes to the old database
would not conflict with all the new ones. It worked well, but the program used is fairly clunky and
surprising with arguably poor documentation so I won't recommend it here.<h2><a id=stringly-typed-systems href=#stringly-typed-systems>Stringly Typed Systems</a></h2><p>One of the changes we hoped to make in all this was reducing the degree to which this system was
<em>stringly</em> typed, which would also reduce the amount of data stored. Stringly typed systems
are systems that use strings for things that should otherwise have much finer semantics on the
possible values they can be. In this case, encoding the
<abbr title="Universally Unique Identifier">UUID</abbr> as a string results in over twice as much
data being stored for no good reason. It also means the system doesn't enforce these actually be
<abbr title="Universally Unique Identifier">UUID</abbr>s, something the unit tests of the system
took copious advantage of.<p>In this system there are just over three hundred test cases. To change the type of the field
containing this <abbr title="Universally Unique Identifier">UUID</abbr> meant that well over three
hundred of them would no longer work. See, <abbr title="Universally Unique Identifier">UUID</abbr>s
are huge. So huge that given the chance, the original authors of this system took it upon themselves
to use jokey strings instead of real <abbr title="Universally Unique Identifiers">UUIDs</abbr> when
creating test cases. This leads to a problem though. Reading and fixing that many tests, trying to
keep straight all the different identifiers and who needs what is a herculean task. Some test cases
have dozens of these values in play and it's essential to their operation that they match. Given
none of these look like <abbr title="Universally Unique Identifier">UUID</abbr>s, the problem of
just finding what strings are now supposed to be
<abbr title="Universally Unique Identifiers">UUIDs</abbr> is intense, so how do you go about fixing
this much code?<p>Well, first, add a couple lines of code to the system that will cause every test that attempts to
create these broken records to fail. This was actually fairly easy. Now you know all the broken test
cases. The next bit is way more fun. Write a wrapper function that you can slap around all these
joke strings that hashes them into authentic
<abbr title="Universally Unique Identifier">UUID</abbr>s. This means you can spend a few hours just
going through every broken test only focused on trying to find the strings that are supposed to be
<abbr title="Universally Unique Identifiers">UUIDs</abbr> and not have to worry about keeping track
of the test's logic too much. Instead, wrap the strings in a friendly
<code>uuid_hash("myJokeID")</code> and move on. So here's such a function I wrote:<div class=no-css></div><pre><code>import hashlib
import uuid

def uuid_hash(val: str) -> str:
	"""
	This is a hack to cover for over 300 broken tests that assumed the IDs were only strings.
	Passing any string will return a UUID v4 that is unique to the input.

	Arguments:
		val: str
			The string to hash to a UUID.

	Returns: str
		A UUID string based on the val passed.
	"""
	val_bytes = val.encode()
	val_hashed = hashlib.md5(val_bytes).digest()
	val_array = bytearray(val_hashed)

	# We need it to be a UUID v4 like: ________-____-4___-[89ab]___-____________
	val_array[6] = val_array[6] &amp; 0b0000_1111 | 0b0100_0000  # :=      4[0-9a-f]
	val_array[8] = val_array[8] &amp; 0b0011_1111 | 0b1000_0000  # := [89ab][0-9a-f]

	val_uuid = str(uuid.UUID(bytes=bytes(val_array)))

	return val_uuid</code></pre><p>Magic! Several hours of mindless coding later and every test is now passing. For a bit of a
breakdown on how it works, there's just three parts. Part 1:<div class=no-css></div><pre><code>val_bytes = val.encode()
val_hashed = hashlib.md5(val_bytes).digest()
val_array = bytearray(val_hashed)</code></pre><p>This converts the given string into bytes and then hashes those bytes to an
<abbr title="Message-Digest Version 5">MD5</abbr> hash (as binary data, not a hex string). Quick
note on the use of <abbr title="Message-Digest Version 5">MD5</abbr>:
<strong>Never use <abbr title="Message-Digest Version 5">MD5</abbr> in security critical code.</strong>
It is completely broken against malicious inputs. Here though, it's a perfectly fine 128 bit digest
hash. All we need is a deterministic mapping from arbitrary strings to
<abbr title="Universally Unique Identifiers">UUIDs</abbr> and this works just fine. Next is part 2:<div class=no-css></div><pre><code>val_array[6] = val_array[6] &amp; 0b0000_1111 | 0b0100_0000
val_array[8] = val_array[8] &amp; 0b0011_1111 | 0b1000_0000</code></pre><p>Here we clobber the four most significant bits (bits 12 through 15) of the
<var>time_hi_and_version</var> field to the value 4 required of the
<abbr title="Universally Unique Identifier">UUID</abbr> version 4 specification. We also clobber
bits 6 and 7 of the <var>clock_seq_hi_and_reserved</var> to zero and one, respectively. We do this
by first using a bitwise and (<code>&</code>) to apply a bitmask over the byte that contains the
directed bits. Using a bitmask, wherever there's a 1, that bit of the original byte will be left as
it is, but any 0s will be set to a 0 because <code>0 & x = 0</code>. Then we use a bitwise or
(<code>|</code>) to set the zeros to 1s where needed.<p>Finally for part 3, we use <code>str(uuid.UUID(bytes=bytes(val_array)))</code> to let the python
<code>uuid</code> module convert those bytes into a <code>uuid.UUID()</code> object which we can
convert to a string using it's <code>uuid.UUID().__repr__()</code> magic method.<p>There's a bit of a problem with this solution though. Now there are thousands of calls to this
function. The tests now take several seconds longer to run than they used to. Well, here's one of
the most beautiful things about Python in my opinion: runtime introspection. Python has amazing
runtime introspection, meaning with just a little more code we can have this code delete itself.
Seriously, here's what we can add just before the return statement:<div class=no-css></div><pre><code>import inspect
import re

…

call_match = re.compile(fr"uuid_hash\(\s*(['\"]){val}(['\"])\s*\)", re.MULTILINE)
with open(inspect.stack()[1].filename, "r+") as fptr:
	content = fptr.read()
	new_content = call_match.sub(f"\g&lt;1&gt;{val_uuid}\g&lt;2&gt;", content)

	fptr.seek(0)
	fptr.truncate()
	fptr.write(new_content)</code></pre><p>The <code>inspect</code> module allows us to get the call stack of the currently running code. This
allows us to find out what file the calling function lives in. Using this, we create a fairly simple
regular expression that searches for a call to <code>uuid_hash()</code> with the value we actually
got and use that to replace that function call with what this function returns. Putting it all
together we get:<div class=no-css></div><pre><code>import hashlib
import inspect
import re
import uuid

def uuid_hash(val: str) -> str:
	"""
	This is a hack to cover for over 300 broken tests that assumed the IDs were only strings.
	Passing any string will return a UUID v4 that is unique to the input.

	Arguments:
		val: str
			The string to hash to a UUID.

	Returns: str
		A UUID string based on the val passed.
	"""
	val_bytes = val.encode()
	val_hashed = hashlib.md5(val_bytes).digest()
	val_array = bytearray(val_hashed)

	# We need it to be a UUID v4 like: ________-____-4___-[89ab]___-____________
	val_array[6] = val_array[6] &amp; 0b0000_1111 | 0b0100_0000  # :=      4[0-9a-f]
	val_array[8] = val_array[8] &amp; 0b0011_1111 | 0b1000_0000  # := [89ab][0-9a-f]

	val_uuid = str(uuid.UUID(bytes=bytes(val_array)))

	call_match = re.compile(fr"uuid_hash\(\s*(['\"]){val}(['\"])\s*\)", re.MULTILINE)
	with open(inspect.stack()[1].filename, "r+") as fptr:
		content = fptr.read()
		new_content = call_match.sub(f"\g&lt;1&gt;{val_uuid}\g&lt;2&gt;", content)

		fptr.seek(0)
		fptr.truncate()
		fptr.write(new_content)

	return val_uuid</code></pre><p><em>chef kiss</em><h2><a id=python-uuid-gotcha href=#python-uuid-gotcha>Python <abbr title="Universally Unique Identifier">UUID</abbr> Gotcha</a></h2><p>One other side tidbit I discovered in all this is that the <code>uuid.UUID.__init__()</code>
initializer is not sufficient for validating a string is a valid
<abbr title="Universally Unique Identifier">UUID</abbr>. A change a colleague added to the code was
to do better input validation on these strings to ensure client applications actually send us
<abbr title="Universally Unique Identifiers">UUIDs</abbr> (something we should have always done, but
I digress). To do this they added effectively:<div class=no-css></div><pre><code>data = {…, "id": str, …}

try:
	uuid.UUID(data["id"])
except ValueError:
	return HTTPError(400, f"Bad id value {data['id']}.")</code></pre><p>I had a hunch this would be pointlessly slow given this at a minimum would be deserializing the
string and allocating a new object to immediately throw it away for essentially no reason. So I
decided to do two things, first I used <code>timeit</code> to verify how this compared to a compiled
regular expression for validation. It's almost three times as fast:<div class=no-css></div><pre><code>import timeit


# Case 1 ]==================================================
setup = """
import uuid

val = "56541e71-532c-4133-92c9-ef519cb460df"
"""

stmt = """
uuid.UUID(val)
"""

time1 = timeit.timeit(stmt, setup)


# Case 2 ]==================================================
setup = """
import re

val = "56541e71-532c-4133-92c9-ef519cb460df"
uuid_re = re.compile(
	"^"
	"[0-9a-f]{8}-"
	"[0-9a-f]{4}-"
	"4[0-9a-f]{3}-"
	"[89ab][0-9a-f]{3}-"
	"[0-9a-f]{12}"
	"$"
)
"""

stmt = """
uuid_re.match(val)
"""

time2 = timeit.timeit(stmt, setup)


# Conclusion ]==============================================
print(f"It's {time1 / time2 :.3} times as fast!")</code></pre><div class=no-css></div><pre><samp>It's 2.83 times as fast!</samp></pre><p>There's another issue though if we dig a bit deeper. If we read the
<a href=https://github.com/python/cpython/blob/3db42fc5aca320b0cac1895bc3cb731235ede794/Lib/uuid.py#L174>source of the <code>uuid</code> module</a>,
we can see the initializer is not designed to securely verify the input. Relevant code:<div class=no-css></div><pre><code>hex = hex.replace('urn:', '').replace('uuid:', '')
hex = hex.strip('{}').replace('-', '')
if len(hex) != 32:
	raise ValueError('badly formed hexadecimal UUID string')
int = int_(hex, 16)</code></pre><p>Here <var>hex</var> is the first argument of the initializer. This code means all sorts of
non-<abbr title="Universally Unique Identifier">UUID</abbr> strings will parse into valid
<code>uuid.UUID()</code> objects. For example,
<samp>{e32682f1-furn:25d----4bd-9-8c21-dbb-8fd-349c18}}}uuid:</samp>. I'm sure there's code out
there that assumes this can be used for input validation. There's probably a whole host of lessons
in here on security like knowing what code you write is actually doing, being clear in your
documentation what your <abbr title="Application Programming Interface">API</abbr> should and should
not be used for, or not using a cannon to kill a fly. I'll just leave drawing those as an exercise
to the reader.<h2><a id=update-brand-new-uuid-versions href=#update-brand-new-uuid-versions>Update: Brand New <abbr title="Universally Unique Identifier">UUID</abbr> Versions?</a></h2><p>I found a fun bit of news to share that I read literally the same day I published this. Turns out
there's a
<a href=https://datatracker.ietf.org/doc/html/draft-peabody-dispatch-new-uuid-format>draft <abbr title="Request For Comments">RFC</abbr></a>
(at the time of writing) to create 3 new <abbr title="Universally Unique Identifier">UUID</abbr>
versions, bringing the total to 8. These new versions are designed specifically to deal with this
problem. Well then… My thanks to <a href=https://peabody.io/>Brad Peabody</a> and Kyzer Davis for
working to standardize this.<p>How do they work? Well, they all start with a timestamp that's big-endian encoded. This means they
all sort lexically by timestamps.<p><em><abbr title="Universally Unique Identifier">UUID</abbr> version 6</em> keeps the Gregorian
timestamps from <abbr title="Universally Unique Identifier">UUID</abbr> version 1 (count of
100-nanosecond intervals since
<time datetime="1582-10-15 00:00:00.00">October 15th, 1582 at 00:00:00.00 <abbr title="Coordinated Universal Time">UTC</abbr></time>)
but reorders the parts of it so it sorts properly. It also replaces use of the
<abbr title="Media Access Control">MAC</abbr> address with random bits. This is designed to be the
most "drop in" for existing <abbr title="Universally Unique Identifier">UUID</abbr> implementations.<p><em><abbr title="Universally Unique Identifier">UUID</abbr> version 7</em> is a 36-bit unix
timestamp with arbitrary precision and a sequence ID followed by random bits. 36-bits for the
timestamp means it has rollover problems on August 20th, 4147 instead of
<a href=https://en.wikipedia.org/wiki/Year_2038_problem><time datetime=2038-01-19>January 19th, 2038</time></a>
as the normal 32-bit variant has. The sequence bits ensure a single system generating multiple
<abbr title="Universally Unique Identifiers">UUIDs</abbr> at the same time keeps the resulting
records in sorted order. Arbitrary precision means it supports any level of sub-seconds down and
passed nanoseconds if you really want that. It does it in such a way that you can even have
different precisions within the same system. This is the version I would personally encourage people
use. It also seems to be the one the authors favour as well.<p><em><abbr title="Universally Unique Identifier">UUID</abbr> version 8</em> is a format designed for
use cases that need a custom timestamp format. By specification it requires you verify you cannot
use versions 1, 6, or 7 before you resort to version 8. It's similar to version 7 but doesn't
require the time be a unix epoch or Gregorian timestamp. It still has the sequence field followed by
random data but that random data can also be node identification data. I mean, if you really want to
share your <abbr title="Media Access Control">MAC</abbr> address (or other node identifiers) despite
the privacy implications of that. Ultimately, version 8 is a kind of roll your own format. It may
have uses if you want identifiers to have more meaning than simply identifying a resource. I'd
personally encourage applying those values as attributes of a record a client can query instead of
packing them into the identifier you share with everyone. That way you can better authorize who can
see what attributes of a record based only on an opaque token, but your use case may vary.</article></main>